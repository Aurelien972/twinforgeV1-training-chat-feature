/**
 * Voice Coach Orchestrator - WebRTC Edition
 * Service central qui coordonne le syst√®me vocal WebRTC
 * Architecture simplifi√©e : audio g√©r√© automatiquement par WebRTC
 * Se concentre sur la gestion des √©v√©nements et des transcriptions
 */

import logger from '../../lib/utils/logger';
import { useUnifiedCoachStore } from '../store/unifiedCoachStore';
import type { VoiceState } from '../store/unifiedCoachStore';
import { openaiRealtimeService } from './openaiRealtimeService';
import { useUserStore } from '../store/userStore';

class VoiceCoachOrchestrator {
  private isInitialized = false;
  private currentCoachMessage = ''; // Accumulation de la transcription du coach

  /**
   * Initialiser l'orchestrateur
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) {
      logger.debug('VOICE_ORCHESTRATOR', 'Already initialized');
      return;
    }

    try {
      logger.info('VOICE_ORCHESTRATOR', 'Initializing voice coach orchestrator (WebRTC mode)');

      // V√©rifier la configuration Supabase
      logger.debug('VOICE_ORCHESTRATOR', 'Checking Supabase configuration');
      const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
      const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY;

      if (!supabaseUrl || !supabaseAnonKey) {
        logger.error('VOICE_ORCHESTRATOR', 'Supabase configuration missing');
        throw new Error('Supabase configuration missing');
      }

      logger.debug('VOICE_ORCHESTRATOR', 'Supabase configuration OK, setting up handlers');

      // Setup event handlers pour l'API Realtime WebRTC
      logger.debug('VOICE_ORCHESTRATOR', 'Setting up Realtime handlers');
      this.setupRealtimeHandlers();

      this.isInitialized = true;

      logger.info('VOICE_ORCHESTRATOR', 'Voice coach orchestrator initialized successfully (WebRTC mode)');
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Initialization failed', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined
      });
      throw error;
    }
  }

  /**
   * D√©marrer une session vocale
   */
  async startVoiceSession(mode: string): Promise<void> {
    const store = useUnifiedCoachStore.getState();

    try {
      logger.info('VOICE_ORCHESTRATOR', 'üöÄ Starting voice session (WebRTC)', { mode });

      // V√©rifier l'√©tat actuel
      if (store.voiceState === 'listening' || store.voiceState === 'speaking') {
        logger.warn('VOICE_ORCHESTRATOR', 'Session already active');
        return;
      }

      // Passer en √©tat connecting
      logger.info('VOICE_ORCHESTRATOR', 'üì° Setting voice state to connecting');
      store.setVoiceState('connecting');

      // Run diagnostics first to identify issues early
      logger.info('VOICE_ORCHESTRATOR', 'üî¨ Running pre-connection diagnostics');
      const { VoiceConnectionDiagnostics } = await import('./voiceConnectionDiagnostics');
      const diagnostics = new VoiceConnectionDiagnostics();
      const results = await diagnostics.runAllTests();

      const failedTests = results.filter(r => !r.passed);
      if (failedTests.length > 0) {
        logger.error('VOICE_ORCHESTRATOR', '‚ùå Diagnostics failed', {
          failedTests: failedTests.map(t => ({ test: t.test, message: t.message }))
        });

        // Show detailed error to user
        const errorMessages = failedTests.map(t => `‚Ä¢ ${t.test}: ${t.message}`).join('\n');
        throw new Error(`Voice connection prerequisites failed:\n\n${errorMessages}\n\nPlease check the console for detailed diagnostics.`);
      }

      logger.info('VOICE_ORCHESTRATOR', '‚úÖ All diagnostics passed');

      // R√©cup√©rer le pr√©nom de l'utilisateur
      const userProfile = useUserStore.getState().profile;
      const firstName = userProfile?.displayName || 'champion';

      // R√©cup√©rer la configuration du mode depuis unifiedCoachStore
      const modeConfig = store.modeConfigs[mode as any];

      if (!modeConfig) {
        throw new Error(`Invalid mode: ${mode}`);
      }

      // Enrichir le system prompt avec le pr√©nom de l'utilisateur
      const personalizedSystemPrompt = `${modeConfig.systemPrompt}\n\nIMPORTANT: L'utilisateur s'appelle ${firstName}. Adresse-toi √† lui par son pr√©nom pour cr√©er une relation plus chaleureuse et personnelle.`;

      logger.info('VOICE_ORCHESTRATOR', 'üë§ Personalized system prompt', { firstName, mode });

      // Connexion √† l'API Realtime WebRTC via l'interface unifi√©e
      // Note: La demande de permissions micro est faite automatiquement par openaiRealtimeService
      logger.info('VOICE_ORCHESTRATOR', 'üåê Connecting to Realtime API via WebRTC');

      // La configuration est pass√©e directement lors de la connexion (instructions)
      await openaiRealtimeService.connect({
        model: 'gpt-realtime-mini', // Updated to cost-efficient realtime model
        voice: 'alloy',
        temperature: 0.8,
        maxTokens: 4096,
        instructions: personalizedSystemPrompt // System prompt personnalis√© avec le pr√©nom
      });
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Realtime API WebRTC connected successfully');

      // CRITIQUE: Configurer la session pour activer la d√©tection vocale et les r√©ponses
      // Le data channel est maintenant garanti d'√™tre ouvert, donc cette configuration sera envoy√©e avec succ√®s
      logger.info('VOICE_ORCHESTRATOR', '‚öôÔ∏è Configuring session with VAD and transcription');
      await openaiRealtimeService.configureSession(personalizedSystemPrompt, mode as any);
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Session configuration confirmed by server');

      // Logger les diagnostics de connexion
      const connectionDiagnostics = openaiRealtimeService.getConnectionDiagnostics();
      logger.info('VOICE_ORCHESTRATOR', 'üîç Connection diagnostics after configuration:', connectionDiagnostics);

      if (!connectionDiagnostics.audioInputActive) {
        logger.warn('VOICE_ORCHESTRATOR', '‚ö†Ô∏è Audio input may not be active - user speech detection might not work');
      }

      // D√©marrer une conversation dans le store
      logger.info('VOICE_ORCHESTRATOR', 'üí¨ Starting conversation in store');
      store.startConversation(mode as any);

      // IMPORTANT: Mettre √† jour le mode de communication sans fermer le panel
      // L'utilisateur doit rester dans la fen√™tre de chat pour voir la conversation
      logger.info('VOICE_ORCHESTRATOR', 'üéôÔ∏è Setting communication mode to voice (realtime)');
      store.setCommunicationMode('voice');
      store.setInputMode('realtime');

      // Le pr√©nom a d√©j√† √©t√© r√©cup√©r√© plus haut

      // Construire le message de bienvenue personnalis√© selon le mode
      const modeWelcomeMessages: Record<string, string> = {
        training: `Salut ${firstName}! Je suis ton coach sportif personnel. Pr√™t √† donner le meilleur de toi-m√™me aujourd'hui?`,
        nutrition: `Salut ${firstName}! Nous voil√† pr√™ts √† optimiser notre alimentation. Qu'est-ce que nous allons travailler aujourd'hui?`,
        fasting: `Salut ${firstName}! Nous commen√ßons une nouvelle session de je√ªne ensemble. Comment nous sentons-nous?`,
        general: `Salut ${firstName}! Nous sommes TwinCoach, ton compagnon de progression. Qu'est-ce que nous allons accomplir aujourd'hui?`,
        'body-scan': `Salut ${firstName}! Regardons ensemble notre √©volution corporelle. Pr√™t √† d√©couvrir nos progr√®s?`
      };

      const welcomeMessage = modeWelcomeMessages[mode] || `Bonjour ${firstName}! Nous sommes l√† pour t'accompagner. Que souhaitons-nous faire aujourd'hui?`;

      // Envoyer le message de bienvenue personnalis√© automatique pour d√©marrer la conversation
      logger.info('VOICE_ORCHESTRATOR', 'üëã Triggering personalized welcome message from coach', { firstName, mode });
      openaiRealtimeService.sendTextMessage(welcomeMessage);

      // Note: Avec WebRTC, l'audio est g√©r√© automatiquement via les tracks
      // Pas besoin de d√©marrer manuellement l'enregistrement

      // Passer en √©tat listening
      logger.info('VOICE_ORCHESTRATOR', 'üëÇ Setting voice state to listening');
      store.setVoiceState('listening');

      logger.info('VOICE_ORCHESTRATOR', '‚úÖ‚úÖ‚úÖ Voice session started successfully - STATE = LISTENING ‚úÖ‚úÖ‚úÖ');
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Failed to start voice session', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined
      });

      logger.error('VOICE_ORCHESTRATOR', '‚ùå CRITICAL: Voice session failed to start');
      store.setVoiceState('error');

      // Message d'erreur d√©taill√©
      let errorMessage = 'Failed to start voice session';
      if (error instanceof Error) {
        if (error.message.includes('permission')) {
          errorMessage = 'Microphone permission required';
        } else if (error.message.includes('connect') || error.message.includes('WebRTC')) {
          errorMessage = 'Unable to connect to voice service';
        } else {
          errorMessage = error.message;
        }
      }

      logger.error('VOICE_ORCHESTRATOR', 'üí• Error message for user', { errorMessage });
      store.setError(errorMessage);

      throw error;
    }
  }

  /**
   * Arr√™ter la session vocale
   */
  async stopVoiceSession(): Promise<void> {
    try {
      logger.info('VOICE_ORCHESTRATOR', 'Stopping voice session');

      // D√©connecter de l'API (qui nettoiera automatiquement l'audio WebRTC)
      openaiRealtimeService.disconnect();

      // Terminer la conversation
      const store = useUnifiedCoachStore.getState();
      store.endConversation();

      // Reset √©tat
      store.setVoiceState('idle');

      // NOTE: Ne pas sortir du mode voice-only ici
      // C'est fait par exitVoiceOnlyMode() dans VoiceSessionMinimal
      // qui ouvre automatiquement le chat avec l'historique

      logger.info('VOICE_ORCHESTRATOR', 'Voice session stopped');
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Error stopping voice session', { error });
    }
  }

  /**
   * Setup des handlers pour l'API Realtime
   */
  private setupRealtimeHandlers(): void {
    // Handler pour les messages re√ßus
    openaiRealtimeService.onMessage((message) => {
      this.handleRealtimeMessage(message);
    });

    // Handler pour les erreurs
    openaiRealtimeService.onError((error) => {
      logger.error('VOICE_ORCHESTRATOR', '‚ùå Realtime API error', { error: error.message, stack: error.stack });

      const store = useUnifiedCoachStore.getState();
      store.setVoiceState('error');
      store.setError(error.message);
    });

    // Handler pour la connexion
    openaiRealtimeService.onConnect(() => {
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Realtime API WebRTC connected successfully');
    });

    // Handler pour la d√©connexion
    openaiRealtimeService.onDisconnect(() => {
      logger.info('VOICE_ORCHESTRATOR', 'üîå Realtime API disconnected');
    });
  }

  // Avec WebRTC, plus besoin de g√©rer manuellement l'audio
  // Tout est automatique via les tracks WebRTC

  /**
   * Traiter les messages de l'API Realtime
   */
  private handleRealtimeMessage(message: any): void {
    const store = useUnifiedCoachStore.getState();

    // Logger TOUS les messages pour diagnostic
    logger.info('VOICE_ORCHESTRATOR', `üì® Received Realtime message: ${message.type}`, {
      type: message.type,
      hasContent: !!message.delta || !!message.transcript,
      hasDelta: !!message.delta,
      hasTranscript: !!message.transcript,
      hasAudio: !!message.audio,
      fullMessage: message
    });

    switch (message.type) {
      // D√©tection du d√©but de parole de l'utilisateur
      case 'input_audio_buffer.speech_started':
        logger.info('VOICE_ORCHESTRATOR', 'üé§ User started speaking - VAD detected speech');
        store.setVoiceState('processing');
        store.setProcessing(true);
        break;

      // D√©tection de la fin de parole de l'utilisateur
      case 'input_audio_buffer.speech_stopped':
        logger.info('VOICE_ORCHESTRATOR', 'üîá User stopped speaking - VAD detected silence');
        // L'√©tat sera mis √† jour quand la transcription arrive
        break;

      // Transcription de l'utilisateur en cours (delta)
      case 'conversation.item.input_audio_transcription.delta':
        if (message.delta) {
          logger.info('VOICE_ORCHESTRATOR', 'üìù User transcription delta', { delta: message.delta });
          store.setCurrentTranscription(store.currentTranscription + message.delta);
        }
        break;

      // Transcription de l'utilisateur compl√®te
      case 'conversation.item.input_audio_transcription.completed':
        if (message.transcript) {
          logger.info('VOICE_ORCHESTRATOR', '‚úÖ User transcription completed', { transcript: message.transcript });
          store.setCurrentTranscription(message.transcript);

          // Ajouter le message utilisateur
          store.addMessage({
            role: 'user',
            content: message.transcript
          });

          // R√©initialiser la transcription courante
          store.setCurrentTranscription('');
        }
        break;

      // Item de conversation cr√©√© (pour tracker les messages)
      case 'conversation.item.created':
        logger.info('VOICE_ORCHESTRATOR', 'üìã Conversation item created', {
          itemId: message.item?.id,
          itemType: message.item?.type,
          itemRole: message.item?.role
        });
        break;

      // D√©but de r√©ponse du coach (audio)
      case 'response.audio.delta':
        // Avec WebRTC, l'audio est jou√© automatiquement
        // On se contente de mettre √† jour l'√©tat
        if (store.voiceState !== 'speaking') {
          logger.info('VOICE_ORCHESTRATOR', 'üîä Coach speaking via WebRTC, setting state to speaking');
          store.setVoiceState('speaking');
          store.setSpeaking(true);
        }
        break;

      // Transcription de la r√©ponse du coach (delta)
      case 'response.audio_transcript.delta':
        if (message.delta) {
          logger.info('VOICE_ORCHESTRATOR', 'üí¨ Coach transcript delta', { delta: message.delta.substring(0, 50) });
          this.currentCoachMessage += message.delta;

          // Mettre √† jour le dernier message ou en cr√©er un nouveau
          const messages = store.messages;
          const lastMessage = messages[messages.length - 1];

          if (lastMessage && lastMessage.role === 'coach') {
            // Mettre √† jour via le store (Zustand handle l'immutabilit√©)
            logger.debug('VOICE_ORCHESTRATOR', 'üîÑ Updating existing coach message');
            // On ne peut pas modifier directement, il faut recr√©er le tableau
            const updatedMessages = messages.slice(0, -1);
            store.clearMessages();
            updatedMessages.forEach(msg => store.addMessage(msg));
            store.addMessage({
              role: 'coach',
              content: this.currentCoachMessage
            });
          } else {
            // Cr√©er un nouveau message du coach
            logger.info('VOICE_ORCHESTRATOR', '‚ûï Creating new coach message');
            store.addMessage({
              role: 'coach',
              content: this.currentCoachMessage
            });
          }
        }
        break;

      // Transcription du coach compl√®te
      case 'response.audio_transcript.done':
        if (this.currentCoachMessage) {
          logger.info('VOICE_ORCHESTRATOR', '‚úÖ Coach transcript completed', {
            fullMessage: this.currentCoachMessage.substring(0, 100) + '...'
          });

          // R√©initialiser l'accumulation
          this.currentCoachMessage = '';
        }
        break;

      // Fin de r√©ponse audio
      case 'response.audio.done':
        logger.info('VOICE_ORCHESTRATOR', '‚úÖ Audio response completed');
        store.setSpeaking(false);
        break;

      // Fin de r√©ponse compl√®te
      case 'response.done':
        logger.info('VOICE_ORCHESTRATOR', '‚úÖ Response done, setting state back to listening');
        store.setVoiceState('listening');
        store.setProcessing(false);
        store.setSpeaking(false);
        break;

      // Erreur
      case 'error':
        logger.error('VOICE_ORCHESTRATOR', '‚ùå Realtime API error from server', {
          errorMessage: message.error?.message,
          errorType: message.error?.type,
          fullMessage: message
        });
        store.setVoiceState('error');
        store.setError(message.error?.message || 'Unknown error');
        break;

      // Session mise √† jour
      case 'session.updated':
        logger.info('VOICE_ORCHESTRATOR', '‚öôÔ∏è Session configuration updated');
        break;

      // D√©but de cr√©ation de r√©ponse
      case 'response.created':
        logger.info('VOICE_ORCHESTRATOR', 'üéØ Response creation started by server');
        store.setVoiceState('processing');
        store.setProcessing(true);
        break;

      default:
        logger.debug('VOICE_ORCHESTRATOR', '‚ùì Unhandled message type', { type: message.type });
    }
  }

  /**
   * Nettoyer l'orchestrateur
   */
  cleanup(): void {
    this.currentCoachMessage = '';
    this.isInitialized = false;
  }

  /**
   * V√©rifier si l'orchestrateur est initialis√©
   */
  get initialized(): boolean {
    return this.isInitialized;
  }
}

// Export singleton
export const voiceCoachOrchestrator = new VoiceCoachOrchestrator();
